#############################
# YASS configuration schema #
#############################

# Definition reference: http://docs.python-cerberus.org/en/stable/

data:
  type: dict
  required: True
  schema:
    # project's root folder, data will be loaded and saved here
    # can be an absolute or relative path
    root_folder:
      type: string
      required: True
    # recordings filename, relative to root folder
    recordings:
      type: string
      required: True
    # channel geometry filename, relative to root folder
    geometry:
      type: string
      required: True

resources:
  type: dict
  required: True
  schema:
    # maximum memory allowed in batch processing, can be a number (bytes)
    # or a string such as 1000MB, 2GB
    max_memory:
      type: [integer, string]
      required: True


recordings:
  type: dict
  required: True
  schema:
    # precision of the recording â€“ must be a valid numpy dtype
    dtype:
      type: string
      required: True
    # recording rate (in Hz)
    sampling_rate:
      type: integer
      required: True
    # number of channels
    n_channels:
      type: integer
      required: True
    # channels spatial radius
    spatial_radius:
      type: integer
      required: True
    # temporal length of wavforms in ms
    spike_size_ms:
      type: float
      required: True
    # recordings format: wide (number of channel x number of observations)
    # or long (number of oservations x number of channels)
    format:
      type: string
      required: True
      allowed: [wide, long]

preprocess:
  type: dict
  schema:
    # apply butterworth filter in the preprocessing step?
    apply_filter:
      type: boolean
      default: True
    # output dtype for transformed data
    dtype:
      type: string
      default: float64
    filter:
      type: dict
      default:
        order: 3
        low_pass_freq: 300
        high_factor: 0.1
      schema:
        # Order of Butterworth filter
        order:
          type: integer
          default: 3
        # Low pass frequency (Hz)
        low_pass_freq:
          type: integer
          default: 300
        # High pass factor (proportion of sampling rate)
        high_factor:
          type: float
          default: 0.1


detect:
  type: dict
  schema:
    # 'nn' for neural net detction, 'threshold' for amplitude threshold detection
    method:
      type: string
      required: True
      allowed: [threshold, nn]
    # number of features in the temporal dimension to use when applying
    # dimensionality reduction
    temporal_features:
      type: integer
      default: 3
    neural_network_detector:
      type: dict
      default:
        # model name, can be any of the models included in yass (detectnet1.ckpt),
        # a relative folder to data.root_fodler (e.g.
        # $ROOT_FOLDER/models/mymodel.ckpt) or an absolute path to a model
        # (e.g. /path/to/my/model.ckpt). In the same folder as your model, there
        # must be a yaml file with the number and size of the filters, the file
        # should be named exactly as your model but with yaml extension
        # see yass/src/assets/models/ for an example
        filename: detect_nn1.ckpt
        # Threshold for spike event detection
        threshold_spike: 0.5
    neural_network_triage:
      type: dict
      default:
        # same rules apply as in neural_network_detector.filename but the
        # yaml file should only contain size (not number)
        filename: triage_nn1.ckpt
        threshold_collision: 0.5
    neural_network_autoencoder:
      type: dict
      default:
        # same rules apply as in neural_network_detector.filename but no
        # yaml file is needed
        filename: ae_nn1.ckpt
    threshold_detector:
      type: dict
      default:
        std_factor: 4

cluster:
  type: dict
  default:
    masking_threshold: [0.9, 0.5]
    n_split: 5
    method: location
    max_n_spikes: 10000
    prior:
      beta: 1
      a: 1
      lambda0: 0.01
      nu: 5
      V: 2
    coreset:
      clusters: 10
      threshold: 0.95
    triage:
      nearest_neighbors: 20
      percent: 0.1
  schema:
    # Masking threshold
    masking_threshold:
      type: list
      default: [0.9, 0.5]
    # Num. of new clusters in split
    n_split:
      type: integer
      default: 5
    # Choose 'location' for location (x and y : 2 features) + main channel 
    # features (n_feature dimensional) as the feature space. Calculates the location 
    # of the events using a weighted average of the power in the main_channel 
    # and neighboring channels.
    # Choose 'neigh_chan' for n_feature x neighboring_channels dimensional feature 
    # space. The feature space is defined by feature summarization of the waveforms 
    # into n_feature dimensional feature space for only the main_channel and the 
    method:
      type: string
      default: location
    # maximum number of spikes per clustering group
    # if the total number of spikes per clustering group exceeds it,
    # it randomly subsample
    max_n_spikes:
        type: integer
        default: 10000

    prior:
      type: dict
      default:
        beta: 1
        a: 1
        lambda0: 0.01
        nu: 5
        V: 2
      schema:
        beta:
          type: integer
          default: 1
        a:
          type: integer
          default: 1
        lambda0:
          type: float
          default: 0.01
        nu:
          type: integer
          default: 5
        V:
          type: integer
          default: 2

    triage:
      type: dict
      default:
        nearest_neighbors: 20
        percent: 0.1
      schema:
        # number of nearest neighbors to consider
        nearest_neighbors:
          type: integer
          default: 20
        # percentage of data to be triaged
        percent:
          type: float
          default: 0.1

    coreset:
      type: dict
      default:
        clusters: 10
        threshold: 0.95
      schema:
        # Num. of clusters
        clusters:
          type: integer
          default: 10
        # distance threshold
        threshold:
          type: float
          default: 0.95

templates:
  type: dict
  default:
    merge_threshold: [0.8, 0.7]

deconvolution:
  type: dict
  default:
    n_rf: 1.5
    threshold_a: 0.3
    threshold_dd: 0
    n_explore: 2 
    upsample_factor: 5
  schema:
    # refractory period violation in time bins
    n_rf:
      type: float
      default: 1.5
    # threshold on template scale
    threshold_a:
      type: float
      default: 0.3
    # threshold on decrease in L2 difference
    threshold_dd:
      type: float
      default: 0
    # size of windows to look consider around spike time for deconv. 
    n_explore:
      type: integer
      default: 2
    # upsampling factor of templates
    upsample_factor:
      type: integer
      default: 5
